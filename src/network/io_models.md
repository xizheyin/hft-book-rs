# I/O 模型演进 (Evolution of I/O Models)

在深入研究 io_uring 和 Kernel Bypass 之前，我们需要先理解操作系统是如何处理网络 I/O 的。为什么我们需要 `epoll`？为什么 `select` 在连接数多的时候会变慢？

这一切都始于一个基本概念：**文件描述符 (File Descriptor, FD)**。在 Linux 中，一切皆文件，网络套接字 (Socket) 也不例外。

## 1. 基础概念：用户态与内核态

为了安全，操作系统将内存划分为 **用户空间 (User Space)** 和 **内核空间 (Kernel Space)**。
- 你的应用程序运行在用户空间。
- 网卡驱动和协议栈 (TCP/IP) 运行在内核空间。

当网卡收到数据包时：
1.  网卡通过 DMA 将数据写入内核缓冲区 (Kernel Buffer)。
2.  网卡向 CPU 发送中断。
3.  内核处理中断，将数据包放入 Socket 的接收队列。
4.  **用户程序发起系统调用 (read/recv)**，CPU 将数据从内核缓冲区 **复制 (Copy)** 到用户缓冲区。

这个“复制”过程，以及伴随的系统调用 (System Call) 开销，是高性能网络编程需要解决的核心问题。

## 2. 阻塞 I/O (Blocking I/O)

最原始的模型。当你调用 `read()` 时，如果内核缓冲区没有数据，你的线程就会被挂起 (Sleep)，直到数据到达。

```rust
// 伪代码
let mut buf = [0u8; 1024];
// 线程在此处阻塞，直到有数据
let n = socket.read(&mut buf).unwrap(); 
process(&buf[..n]);
```

- **优点**：编程简单，数据来了就处理。
- **缺点**：无法处理并发。如果要处理 10000 个连接，就需要 10000 个线程。线程切换开销巨大。

## 3. 非阻塞 I/O (Non-blocking I/O)

我们可以将 Socket 设置为非阻塞模式 (`O_NONBLOCK`)。此时调用 `read()`，如果没数据，内核会立即返回 `EWOULDBLOCK` 错误，而不是挂起线程。

```rust
socket.set_nonblocking(true).unwrap();

loop {
    match socket.read(&mut buf) {
        Ok(n) => process(&buf[..n]),
        Err(e) if e.kind() == std::io::ErrorKind::WouldBlock => {
            // 没数据，稍后再试
            std::thread::yield_now(); 
        }
        Err(e) => panic!("IO Error: {}", e),
    }
}
```

- **优点**：单线程可以管理多个连接（轮询）。
- **缺点**：**忙轮询 (Busy Polling)** 会导致 CPU 空转率 100%。如果要把 CPU 让给别人 (`yield` 或 `sleep`)，又会引入延迟。

我们需要一种机制，让内核告诉我们：“哪些 Socket 有数据了？”

## 4. I/O 多路复用 (I/O Multiplexing)

这是现代网络服务的基石。我们把一组 FD 交给内核，让内核帮我们要么挂起，要么告诉我们要读哪个。

### 4.1 `select` (1983)

最古老的接口。

- **工作原理**：你传给内核一个 `fd_set` (位图)，内核遍历检查这些 FD 的状态。如果有就绪的，内核修改位图返回。
- **致命缺陷**：
    1.  **数量限制**：默认只能监控 1024 个 FD (`FD_SETSIZE`)。
    2.  **O(N) 开销**：每次调用 `select`，都需要把整个 FD 集合从用户态拷贝到内核态。内核需要遍历所有 FD。即使只有一个 FD 就绪，你也得遍历整个集合才知道是哪个。

### 4.2 `poll` (1997)

改进了 `select`。

- **改进**：使用链表/数组存储 FD，消除了 1024 的数量限制。
- **遗留问题**：依然是 **O(N)** 的。如果有 10 万个连接，只有 1 个活跃，`poll` 依然要扫描这 10 万个项。

### 4.3 `epoll` (2002)

Linux 2.6 引入的革命性技术。它是 **O(1)** 的。

**核心机制**：
1.  **`epoll_create`**: 在内核创建一个 `eventpoll` 对象。
2.  **`epoll_ctl`**: 添加/删除/修改要监控的 FD。内核使用 **红黑树 (Red-Black Tree)** 来管理这些 FD，增删查效率为 O(log N)。
3.  **回调机制**: 当网卡收到数据，中断处理程序会查找红黑树，找到对应的 FD，并将其加入到一个 **就绪链表 (Ready List)** 中。
4.  **`epoll_wait`**: 用户调用此函数，内核只需检查“就绪链表”是否为空。如果不为空，直接返回链表中的项。

**为什么 epoll 快？**
- 不需要每次都把所有 FD 传给内核（`epoll_ctl` 只需要调用一次）。
- 不需要遍历所有 FD，只处理活跃的 (Ready List)。
- 哪怕你监控 100 万个连接，只要同一时刻只有 10 个活跃，`epoll_wait` 就只返回这 10 个，效率与总连接数无关。

### 4.4 触发模式：LT vs ET

`epoll` 有两种工作模式：

1.  **水平触发 (Level Triggered, LT)** - 默认模式
    - 只要缓冲区里还有数据，每次调用 `epoll_wait` 都会通知你。
    - 类似于 `select`/`poll` 的行为。
    - **安全，不易丢数据**。

2.  **边缘触发 (Edge Triggered, ET)** - 高速模式
    - 只有数据**从无到有**（或状态变化）的那一瞬间，才会通知你一次。
    - 如果你收到通知后没把缓冲区读空，下次 `epoll_wait` **不会**再通知你，剩下的数据就“死”在缓冲区里了，直到新数据到达触发下一次边缘。
    - **高效**：减少了系统调用次数。
    - **危险**：必须配合非阻塞 I/O 循环读取，直到 `EWOULDBLOCK`。

## 5. 为什么 HFT 还需要超越 Epoll？

虽然 `epoll` 已经非常高效（Nginx, Redis, Netty 都基于它），但在微秒级竞争中，它仍有瓶颈：

1.  **系统调用开销**：`epoll_wait` 是系统调用。从用户态切换到内核态，再切换回来，至少需要几百纳秒甚至 1 微秒。如果每秒处理 100 万个包，光是系统调用的开销就占满了 CPU。
2.  **内存拷贝**：数据依然需要从内核缓冲区 `copy` 到用户缓冲区。
3.  **中断风暴**：高吞吐量下，网卡频繁中断 CPU，导致上下文切换频繁。

为了解决这些问题，我们引入了 **Kernel Bypass** 技术（DPDK, OpenOnload）和新一代异步 I/O 接口 **io_uring**。

- **io_uring**: 允许用户态和内核态通过共享内存环形队列提交和完成任务，批量处理系统调用，甚至实现零系统调用（Polled Mode）。
- **Kernel Bypass**: 完全绕过内核，用户态程序直接接管网卡，实现真正的零拷贝和零中断。

下一章，我们将深入探讨 **io_uring**。
