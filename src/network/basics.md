# 网络协议栈基础 (Network Stack Basics)

在高频交易的世界里，网络就是生命线。如果你的代码执行只需 100 纳秒，但网络传输花了 50 微秒，那么你的优化就毫无意义。

本章将深入剖析操作系统网络栈的开销，解释为什么 TCP 在 HFT 中往往是次优选择，以及 UDP 组播（Multicast）为何成为行情数据的标准载体。

## 1. 从网卡到用户态：数据的漫长旅程

当一张网卡（NIC）收到一个以太网帧时，它需要经过一系列繁琐的步骤才能到达你的 Rust 程序。让我们跟踪一个 UDP 数据包的旅程：

1.  **PHY/MAC 层**: 网卡芯片接收光信号/电信号，解码为数字帧。
2.  **DMA (Direct Memory Access)**: 网卡通过 PCIe 总线，将数据直接写入主存中的 **Ring Buffer**（Rx Ring）。
3.  **硬中断 (Hard IRQ)**: 网卡向 CPU 发送中断信号，告诉它“有数据来了”。
4.  **软中断 (SoftIRQ/NAPI)**: 内核响应硬中断，挂起当前任务，启动软中断处理程序（ksoftirqd）。
5.  **协议栈处理**: 内核解析 Ethernet header -> IP header -> UDP header，进行校验和计算、路由查找、防火墙规则检查 (iptables)。
6.  **Socket Buffer (sk_buff)**: 数据被放入对应 Socket 的接收队列。
7.  **上下文切换 (Context Switch)**: 唤醒在该 Socket 上阻塞等待的用户线程。
8.  **数据拷贝 (User Copy)**: 数据从内核空间拷贝到用户空间的 Buffer。

**HFT 的痛点**:
*   **中断开销**: 每秒百万级的数据包会触发百万次中断，导致 CPU 频繁上下文切换，产生巨大的 jitter。
*   **内存拷贝**: 内核到用户的拷贝是昂贵的，且污染 CPU Cache。
*   **调度延迟**: 即使数据到了，操作系统也不一定通过调度器立即执行你的线程。

这就是为什么我们需要 **Kernel Bypass**（如 DPDK, Solarflare OpenOnload），它能让我们直接在用户态轮询网卡的 Ring Buffer，跳过上述第 3-8 步。

## 2. TCP: 可靠性的代价

传输控制协议 (TCP) 提供了可靠、有序的字节流传输，但在 HFT 中，它有几个致命弱点：

### 2.1 队头阻塞 (Head-of-Line Blocking)
如果 TCP 窗口中的一个数据包丢失，后续所有已到达的数据包都必须等待重传。在接收行情数据时，这是不可接受的——我们宁愿丢弃那个过时的数据包，也不愿让最新的价格排队等待。

### 2.2 Nagle 算法与 Delayed ACK
*   **Nagle 算法**: 为了减少小包拥塞，TCP 会攒够一定数据量才发送。**必须禁用 (`TCP_NODELAY`)**。
*   **Delayed ACK**: 接收方为了合并 ACK，会延迟发送确认。这会增加 RTT 估算值。

### 2.3 慢启动与拥塞控制
TCP 的拥塞窗口 (Congestion Window) 机制在应对突发流量（如非农数据发布瞬间）时，可能会错误地限制发送速率。

## 3. UDP: 速度与危险

用户数据报协议 (UDP) 是无连接、不可靠的，但它没有握手开销，没有重传机制，没有拥塞控制。

### 3.1 组播 (Multicast)
交易所通常使用 UDP 组播来分发市场数据。
*   **原理**: 交易所只发一份数据到交换机，交换机根据 IGMP 协议，将数据复制给所有订阅了该组播组的机器。
*   **IGMP**: 主机告诉路由器“我想加入组 239.1.1.1”。
*   **A/B Feed**: 为了应对 UDP 丢包，交易所通常会发两路完全相同的数据流（Feed A 和 Feed B）。客户端需要同时监听两路，通过序列号（Sequence Number）进行去重和补缺。

### 3.2 丢包处理
在 UDP 中，丢包是常态。你的程序必须能够：
1.  **检测丢包**: 检查序列号是否连续（如收到 1, 2, 4，说明 3 丢了）。
2.  **重构数据**: 尝试从 Feed B 找回丢失的包。
3.  **重传请求 (Snapshot/Retransmission)**: 如果 A/B 都丢了，通过 TCP 连接向交易所请求快照或重传（通常很慢，是最后的救命稻草）。

## 4. 硬件卸载 (Hardware Offload)

现代网卡非常智能，可以帮 CPU 干活：
*   **Checksum Offload**: 网卡计算 IP/UDP/TCP 校验和，CPU 只需要信任它。
*   **TSO (TCP Segmentation Offload)**: CPU 给网卡一个 64KB 的大包，网卡自动切分成多个 MTU 大小的小包发送。
*   **RSS (Receive Side Scaling)**: 网卡根据 IP/Port 哈希，将流量分发到不同的 Rx Ring，由不同的 CPU 核心处理。

**实战建议**: 在 HFT 中，我们通常**禁用 TSO/LRO**，因为切包逻辑不可控可能增加延迟；但我们会**精心配置 RSS**，确保特定行情的流量只进入特定的 CPU 核心（Core Pinning）。
